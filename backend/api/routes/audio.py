"""Audio API routes."""

import logging
from datetime import datetime
from typing import List

from fastapi import APIRouter, Depends, HTTPException
from fastapi.responses import StreamingResponse
import json

from backend.models.schemas import (
    AudioGenerateRequest,
    AudioGenerateResponse,
    AudioListResponse,
    AudioMetadata,
    ScriptGenerateRequest,
    ScriptGenerateResponse,
    VoiceOption,
    ErrorResponse
)
from backend.services.audio_service import AudioService, get_audio_service
from backend.services.elevenlabs_service import ElevenLabsTTSError

router = APIRouter(prefix="/api/audio", tags=["audio"])


@router.post(
    "/generate-script",
    response_model=ScriptGenerateResponse,
    responses={404: {"model": ErrorResponse}, 500: {"model": ErrorResponse}},
)
async def generate_script(
    request: ScriptGenerateRequest, service: AudioService = Depends(get_audio_service)
):
    """Generate script from knowledge document."""
    try:
        result = await service.generate_script(
            knowledge_id=request.knowledge_id,
            model_name=request.model_name,
            custom_prompt=request.custom_prompt,
            template_config=request.template_config
        )
        return ScriptGenerateResponse(
            script=result["script"],
            knowledge_id=request.knowledge_id,
            model_used=result["model_used"],
            generated_at=datetime.utcnow(),
            generation_error=result.get("generation_error"),
        )
    except ValueError as e:
        # Document not found
        raise HTTPException(status_code=404, detail=str(e))
    # Other exceptions bubble up to global handler


@router.post(
    "/generate-script-stream",
    response_class=StreamingResponse,
    responses={404: {"model": ErrorResponse}, 500: {"model": ErrorResponse}},
)
async def generate_script_stream(
    request: ScriptGenerateRequest, service: AudioService = Depends(get_audio_service)
):
    """Stream script generation with Server-Sent Events.
    
    This endpoint streams tokens as they are generated by the LLM,
    keeping the connection alive and providing real-time feedback.
    This avoids timeout issues with large documents.
    
    Events are formatted as SSE:
    - data: {"type": "token", "content": "..."}
    - data: {"type": "complete", "script": "...", "model_used": "..."}
    - data: {"type": "error", "message": "..."}
    """
    async def event_generator():
        try:
            async for event in service.generate_script_stream(
                knowledge_id=request.knowledge_id,
                model_name=request.model_name,
                custom_prompt=request.custom_prompt,
                template_config=request.template_config
            ):
                # Format as Server-Sent Event
                yield f"data: {json.dumps(event)}\n\n"
        except Exception as e:
            error_event = {"type": "error", "message": str(e)}
            yield f"data: {json.dumps(error_event)}\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # Disable nginx buffering
        }
    )


@router.post(
    "/generate",
    response_model=AudioGenerateResponse,
    responses={
        400: {"model": ErrorResponse},
        502: {"model": ErrorResponse},
        500: {"model": ErrorResponse},
    },
)
async def generate_audio(
    request: AudioGenerateRequest, service: AudioService = Depends(get_audio_service)
):
    """Generate audio from script."""
    # Exceptions bubble up to global handler which maps ElevenLabs errors correctly
    metadata = await service.generate_audio(
        script=request.script,
        voice_id=request.voice_id,
        knowledge_id=request.knowledge_id,
    )
    return AudioGenerateResponse(
        audio_id=metadata.audio_id,
        audio_url=metadata.audio_url,
        knowledge_id=metadata.knowledge_id,
        voice_id=metadata.voice_id,
        duration_seconds=metadata.duration_seconds,
        script=metadata.script,
        created_at=metadata.created_at,
    )


@router.get(
    "/{knowledge_id}",
    response_model=AudioListResponse,
    responses={500: {"model": ErrorResponse}},
)
async def get_audio_files(
    knowledge_id: str, service: AudioService = Depends(get_audio_service)
):
    """Get audio files for a knowledge document."""
    audio_files = await service.get_audio_files(knowledge_id=knowledge_id)
    return AudioListResponse(
        audio_files=audio_files,
        total_count=len(audio_files)
    )


@router.get(
    "/voices/list",
    response_model=List[VoiceOption],
    responses={502: {"model": ErrorResponse}, 500: {"model": ErrorResponse}},
)
async def get_available_voices(service: AudioService = Depends(get_audio_service)):
    """Get available voices."""
    return service.get_available_voices()
